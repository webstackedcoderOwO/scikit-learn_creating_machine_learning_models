# -*- coding: utf-8 -*-
"""scikit-learn_creating_machine_learning_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cbEvxczu_eliYboraz3G2Tfz5Cpmp74L

# **Scikit-learn : Creating Machine Learning Models**

## **Scikit-learn Introduction**

**Q. What is Scikit-leann ?**  
**Soln.** Scikit-learn (Sklearn) is the most useful and robust library for machine learning in Python. It provides a selection of efficient tools for machine learning and statistical modeling including classification, regression, clustering and dimensionality reduction via a consistence interface in Python. This library, which is largely written in Python, is built upon NumPy, SciPy and Matplotlib.

To Know More about this Refer to the Documentations  

[Github](https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-2-data-science-and-ml-tools/introduction-to-scikit-learn-video.ipynb)  
[Introduction about Scikit-learn](https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-2-data-science-and-ml-tools/introduction-to-scikit-learn.ipynb)  
[Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  
[Scikit-learn Machine Learning Map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

**What are we going to Cover ?**
- An end-to-end Scikit-learn workflow
- Getting Data Ready (to be used with machine learning models)
- Choosing a Machine Learning Model
- Fitting a Model to the Data (Learning Patterns)
- Making Predictions with a Model (Using Patterns)
- Evaluating Model Predictions
- Improving Model Predictions
- Saving and Loading Models

## **Refresher : What is Machine Learning ?**

**Q. What is Machine Learning ?**  
**Soln.** Machine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.

## **Typical Scikit-learn Workflow**

### **Introduction to Scikit-learn**

This notebook demonstrates some of the useful functions of the beautiful Scikit-learn library

What we are going to Cover :
 0. An end-to-end Scikit-Learn Workflow
 1. Getting the Data Ready
 2. Choose the Right Estimator/Algorithm for our Problems
 3. Fit the Model/Algorithm and use it to make predictions on our Data
 4. Evaluating the Model
 5. Improve a Model
 6. Save and Load a trained Model
 7. Putting it all Together

### **0. An end-to-end Scikit-Learn Workflow**
"""

# 1. Get the Data Ready
import pandas as pd
import numpy as np
heart_disease = pd.read_csv("/content/heart-disease.csv")
heart_disease

# create X (features Matrix)
X = heart_disease.drop("target", axis=1)

# create y (labels)
y = heart_disease["target"]

# 2. Choose the Right Model and Hyperparameters
from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()

# We will keep the default hyperparameters
clf.get_params()

# 3. Fit the Model to the training Data
from sklearn.model_selection import train_test_split

# 80% of the data will be used for training for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf.fit(X_train, y_train);

# Make a Prediction
y_preds = clf.predict(X_test)
y_preds

y_test

# 4. Evaluate the Model on Training Data and Test Data
clf.score(X_train, y_train)

clf.score(X_test, y_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(classification_report(y_test, y_preds))

confusion_matrix(y_test, y_preds)

accuracy_score(y_test, y_preds)

# 5. Improve a Model
# Try different amout of n_estimators
np.random.seed(42)
for i in range(10, 100, 10):
  print(f"Trying Model with {i} estimators...")
  clf = RandomForestClassifier(n_estimators=i).fit(X_train, y_train)
  print(f"Model accuracy on test set: {clf.score(X_test, y_test) * 100:.2f}%")
  print(" ")

# Save a Model and Load it
# pickle is used to save our ml model
import pickle
pickle.dump(clf, open("random_forest_model_1.pkl", "wb"))

# to load a ml model
loaded_model = pickle.load(open("/content/random_forst_model_1.pkl", "rb"))
loaded_model.score(X_test, y_test)

"""To ignore the Warnings Use:  
import warnings  
warnings.filterwarnings("ignore")

## **Getting your Data Ready : Splitting your Data**
"""

# Standard Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""**Getting Data Ready to be Used with Machine Learning**  
Three Main things we have to do:
  1. Split the Data into features and labels (usually `X` and `y`)
  2. Filling and also called imputing or disregarding missing values.
  3. Converting non numerical values to numerical values also known as feature encoding
"""

heart_disease.head()

# we are going to remove the target column from out heart_disease dataset
X = heart_disease.drop("target", axis=1)
X.head()

# we are making taking the target column and storing it in y
y = heart_disease["target"]
y.head()

# now we need the split data into training and test splits
from sklearn.model_selection import train_test_split

# it will split arrays or matrices into random train and test subsets
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2)

X_train.shape, X_test.shape, y_test.shape, y_train.shape

X.shape

len(heart_disease)

"""## **Getting your Data Ready : Convert Data to Numbers**

**1.1 Make sure its all Numerical**
"""

car_sales = pd.read_csv("/content/car-sales-extended.csv")
car_sales.head()

len(car_sales)

car_sales.dtypes

# split data into X and y
X = car_sales.drop("Price", axis=1)
y = car_sales["Price"]

# Split into training or test 
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2)

# turn the categories into numbers
# one hot encoding is a process used to turn categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot",
                                  one_hot,
                                  categorical_features)],
                                  remainder="passthrough")

transformed_X = transformer.fit_transform(X)
transformed_X

pd.DataFrame(transformed_X)

dummies = pd.get_dummies(car_sales[["Make", "Colour", "Doors"]])
dummies

# build machine learning model
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
#model.fit(X_train, y_test)
#model.score(X_test, y_test)

# lets refit the model
np.random.seed(42)
X_train, X_test, y_train, y_test = train_test_split(transformed_X,
                                                    y,
                                                    test_size=0.2)
model.fit(X_train, y_train)

X.head()

model.score(X_test, y_test)

"""## **Getting your Data Ready : Handling Missing Values with Python**

**1.2 What if their are Missing Values ?**  
**i)**  Fill them with some value (also known as imputation)  
**ii)** Remove the Samples with missing data Altogether.
"""

# import car sales missing data
car_sales_missing = pd.read_csv("/content/car-sales-extended-missing-data.csv")

car_sales_missing.head()

# shows how many missing values in each column
car_sales_missing.isna().sum()

# lets create X and y
X = car_sales_missing.drop("Price", axis=1)
y = car_sales_missing["Price"]

"""**Option1: Fill missing Data with Pandas**"""

# Fill the "Make" column
car_sales_missing["Make"].fillna("missing", inplace=True)

# Fill the "Colour" column
car_sales_missing["Colour"].fillna("missing", inplace=True)

# Fill the "Odometer (KM)" column
car_sales_missing["Odometer (KM)"].fillna(car_sales_missing["Odometer (KM)"].mean(), inplace=True)

# Fill the "Doors" column
car_sales_missing["Doors"].fillna(4, inplace=True)

# check our datafram again
car_sales_missing.isna().sum()

# remove rows with missing price values
car_sales_missing.dropna(inplace=True)

car_sales_missing.isna().sum()

X

X = car_sales_missing.drop("Price", axis=1)
y = car_sales_missing["Price"]

# lets try and convert our data to numbers
# turn the categories into numbers
# one hot encoding is a process used to turn categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot",
                                  one_hot,
                                  categorical_features)],
                                  remainder="passthrough")

transformed_X = transformer.fit_transform(car_sales_missing)
transformed_X

"""## **Getting your Data Ready : Handling Missing Values with Scikit-learn**

**Option 2 : Fill missing values with Scikit-Learn**
"""

car_sales_missing = pd.read_csv("/content/car-sales-extended-missing-data.csv")
car_sales_missing.isna().sum()

# here we are saying remove the na values in the dataframe of given column
car_sales_missing.dropna(subset=["Price"], inplace=True)
car_sales_missing.isna().sum()

# split into X and y
X = car_sales_missing.drop("Price", axis=1)
y = car_sales_missing["Price"]

# fill missing values with scikit-learn
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

# fill categorical values with missing and numerical values with mean
cat_imputer = SimpleImputer(strategy="constant", fill_value="missing")
door_imputer = SimpleImputer(strategy="constant", fill_value=4)
num_imputer = SimpleImputer(strategy="mean")

# define columns
cat_features = ["Make", "Colour"]
door_feature = ["Doors"]
num_features = ["Odometer (KM)"]

# create an imputer (something that fills missing data)
# "name of imputer" which imputer is applying on which column 
imputer = ColumnTransformer([
          ("cat_imputer", cat_imputer, cat_features),
          ("door_imputer", door_imputer, door_feature),
          ("num_imputer", num_imputer, num_features)
                             ])

# transform the data
filled_X = imputer.fit_transform(X)
filled_X

car_sales_filled = pd.DataFrame(filled_X,
                                columns=["Make", "Colour", "Doors", "Odometer (KM)"])
car_sales_filled.head()

car_sales_filled.isna().sum()

# lets try and convert our data to numbers
# turn the categories into numbers
# one hot encoding is a process used to turn categories into numbers
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

categorical_features = ["Make", "Colour", "Doors"]
one_hot = OneHotEncoder()
transformer = ColumnTransformer([("one_hot",
                                  one_hot,
                                  categorical_features)],
                                  remainder="passthrough")

transformed_X = transformer.fit_transform(car_sales_filled)
transformed_X

# now we have got our data as numbers and filled(no missing values)
# now lets fit the model
np.random.seed(42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(transformed_X,
                                                    y,
                                                    test_size=0.2)
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train, y_train)
model.score(X_test, y_test)

"""## **Choosing the Right Model for your Data**

**Choosing the right estimator/ algorithm for Problem**  
Some things to Note :  
- sklearn refers to machine learning models, algorithms as estimators
- classification problem - predicting a category (heart disease or not)
- Sometimes you will see `clf` (short for classifier) used as classification estimator
- Regression problem - predicting number (selling price of car)

Refer to [Scikit-Learn Map](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

**2.1 Picking a Machine Learning model for Regression Problem**  
Finding the Datasets 
- [Toy Datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)
- [Real World DataSets](https://scikit-learn.org/stable/datasets.html)
"""

# Get california housing dataset
from sklearn.datasets import fetch_california_housing
housing = fetch_california_housing()
housing

housing_df = pd.DataFrame(housing["data"], columns=housing["feature_names"])
housing_df

housing_df["target"] = housing["target"]
housing_df.head()

housing_df

"""** The Code Throwing an ERROR**  
**The Code should be :**  

**Import algorithm**  
from sklearn.linear_model import Ridge

**Setup Random Seed**  
np.random.seed(42)

**Create the Data**  
X = housing_df.drop("target", axis=1)  
y = housing_df["target"]        
*Median House Price in $100000*

**Split into train and test sets**  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  

**Instantiate and fit the Model**  
model = Ridge()  
model.fit(X_train, y_train)  

**Check the score of the model on the test set**  
model.score(X_test, y_test)  

**Output should Be**  
0.5785.....

## **Choosing Right Model for your Data 2 (Regresion)**

What if `Ridge` didnot work or the score did not fit our needs ?  
Well we could try Different Model...  
How About we try an ensemble Model (an ensemble is a combination of smaller models to try and make better predictions than just single model)  
Sklearn ensemble models can be found in  
https://scikit-learn.org/stable/modules/ensemble.html
"""

# Import RandomForestRegressor model Class from ensemble module
from sklearn.ensemble import RandomForestRegressor

# setup random seed
np.random.seed(42)

# create the data
X = housing_df.drop("target", axis=1)
y = housing_df["target"]

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# create random forest model
model = RandomForestRegressor()
model.fit(X_train, y_train)

# check the score of the model (on the test set)
model.score(X_test, y_test)

"""## **Choosing The Right Model For Your Data 3 (Classification)**

**Choosing an estimator for a Classification Problem**
"""

heart_diseas = pd.read_csv("/content/heart-disease.csv")
heart_disease.head()

len(heart_disease)

"""Consulting the Map and It says try the `LinearSVC` """

# import the LinearSVC estimator class
from sklearn.svm import LinearSVC

# setup random seed
np.random.seed(42)

# make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# instantiate LinearSVC
clf = LinearSVC()
clf.fit(X_train, y_train)

# Evaluate the LinearSVC
clf.score(X_test, y_test)

"""**Using the RandomForestClassifier Model**"""

# import the RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

# setup random seed
np.random.seed(42)

# make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# instantiate RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_train, y_train)

# Evaluate the RandomForestClassifier
clf.score(X_test, y_test)

"""Note :
- If you have structured data use the ensemble methods
- If you have unstructured data use the Deep Learning or transfer learning.

## **Fitting a Model to a Data**

**Fitting the Model to the Data**  
Different Names for :  
`X` = Features, Feature Variables, Data  
`y` = Labels, Targets, Target Variables
"""

# import the RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

# setup random seed
np.random.seed(42)

# make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# instantiate RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# fit the model to the data
clf.fit(X_train, y_train)

# Evaluate the RandomForestClassifier
clf.score(X_test, y_test)

X.head()

y.head()

"""## **Making Predictions With Our Model**

**There are two ways to make Predictions**
- `predict()`
- `predict_proba()`
"""

X_test.head()

clf.predict(X_test)

np.array(y_test)

# compare predictions to truth labels to evaluate the model
y_preds = clf.predict(X_test)
np.mean(y_preds == y_test)

clf.score(X_test, y_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_preds)

"""**Make predictions with `predict_proba()`**

## **`predict()` vs `predict_proba()`**
"""

# `predict_proba()` returns probabilities of a classification label
clf.predict_proba(X_test[:5])

# lets use `predict()` on the same data
# `predict` returns single values
clf.predict(X_test[:5])

"""## **Making Predictions with our Model (Regression)**

**`predict()` can be also used in Regression Model**
"""

housing_df.head()

from sklearn.ensemble import RandomForestRegressor
np.random.seed(42)

# create the data
X = housing_df.drop("target", axis=1)
y = housing_df["target"]

# split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# create model instance
model = RandomForestRegressor()

# fit the model to the data 
model.fit(X_train, y_train)

# make predictions 
y_preds = model.predict(X_test)

y_preds[:10]

np.array(y_test[:10])

len(y_preds)

len(y_test)

# Compare the predictions to the truth
# this shows our model is 0.32% different than the original
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_test, y_preds)

housing_df["target"]

"""## **Evaluating a Machine Learning Model (Score) Part 1**

**There are three ways to evaluate Scikit-Learn models/estimatores :**  
1. Estimator's built in `score()` method.
2. The `scoring` parameter
3. Problem specific metric functions

**Evaluating Model with `score` Method**
"""

# import the RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

# setup random seed
np.random.seed(42)

# make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# instantiate RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# fit the model to the data
clf.fit(X_train, y_train)

# Evaluate the RandomForestClassifier
clf.score(X_test, y_test)

# the highest value for score method is 1.0 and lowest is 0.0
# `clf.score` return the mean accuracy on the given test data and labels
clf.score(X_train, y_train)

clf.score(X_test, y_test)

"""## **Evaluating a Machine Learning Model (Score) Part 2**

**Let's use the `score()` method for regression model**
"""

# Import RandomForestRegressor model Class from ensemble module
from sklearn.ensemble import RandomForestRegressor

# setup random seed
np.random.seed(42)

# create the data
X = housing_df.drop("target", axis=1)
y = housing_df["target"]

# split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# you can change n_estimators for better accuracy
# create random forest model
model = RandomForestRegressor(n_estimators=50)
model.fit(X_train, y_train)

# model.score automates the prediction of your data using X_test and compares it with Y_test
# the default score() evaluation metrics is r_squared for regressing algorithm 
model.score(X_test, y_test)

housing_df.head()

y_test.mean()

"""## **Evaluating a Machine Learning Model 2 (Cross Validation)**"""

from sklearn.model_selection import cross_val_score

# import the RandomForestClassifier
from sklearn.ensemble import RandomForestClassifier

# setup random seed
np.random.seed(42)

# make the data
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# instantiate RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)

# fit the model to the data
clf.fit(X_train, y_train)

clf.score(X_test, y_test)

# Cross_val_score is a method which runs cross validation on a dataset to test whether the model can generalise over the whole dataset. 
# cv create the given number of versions/splits on our data and test our data on each versions
cross_val_score(clf, X, y, cv=5)

cross_val_score(clf, X, y, cv=10)

np.random.seed(42)

# single training and test split
clf_single_score = clf.score(X_test, y_test)

# take the mean of 5 fold cross validation score
clf_cross_val_score = np.mean(cross_val_score(clf, X, y, cv=5))

# compare the two
clf_single_score, clf_cross_val_score

"""## **Evaluating a Classification Model 1 (Accuracy)**

**Classification Model Metrics**  
1. Accuracy
2. Area under ROC Curve
3. Confusion Matrix
4. Classification Report

### **Accuracy**
"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

clf = RandomForestClassifier()
cross_val_score = cross_val_score(clf, X, y, cv=5)

np.mean(cross_val_score)

print(f"Heart Disease Classifier Cross Validated Accuracy : {np.mean(cross_val_score) * 100:.2f}%")

"""## **Evaluating a Classification Model 2 (ROC Curve)**

### **Area under Receiver Operating Characteristic Curve (AUC or ROC Curve)**

**ROC Curves are a Comparison of a Model's true Positive Rate (tpr) versus a models False positive rate (fpr)**
- **True Positive** : Model Predicts 1 when truth is 1
- **False Positive** : Model Predicts 1 when truth is 0
- **True Negative** : Model Predicts 0 when truth is 0
- **False Negative** : Model Predicts 0 when truth is 1
"""

# creating and X_test data etc
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.metrics import roc_curve

# fit the classifier
clf.fit(X_train, y_train)

# make predictions with probabilities
y_probs = clf.predict_proba(X_test)

y_probs[:10], len(y_probs)

y_probs_positive = y_probs[:, 1]
y_probs_positive

y_probs_positive[:10]

# caalculate the fpr, tpr and thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)

# check the false positive rates
fpr

"""## **Evaluating a Classification Model 3 (ROC Curve)**"""

# create a function for plotting ROC Curve
import matplotlib.pyplot as plt
def plot_roc_curve(fpr, tpr):

  # Plot ROC curver
  plt.plot(fpr, tpr, color="orange", label="ROC")

  # plot line with no predictive power (baseline)
  plt.plot([0, 1], [0, 1], color="darkblue", linestyle="dashed", label="Guessing")

  # customize the plot
  plt.xlabel("False Positive Rate (fpr)")
  plt.ylabel("True Positive Rate (tpr)")
  plt.title("Receiver Operating Characteristic Curve (ROC)")
  plt.legend()
  plt.show()

plot_roc_curve(fpr, tpr)

from sklearn.metrics import roc_auc_score
roc_auc_score(y_test, y_probs_positive)

# plot perfect roc curve and auc score
fpr, tpr, thresholds = roc_curve(y_test, y_test)
plot_roc_curve(fpr, tpr)

# perfect auc score
roc_auc_score(y_test, y_test)

"""## **Evaluating a Classification Model 4 (Confusion Matrix)**

### **Theoritical Details about Confusion Matrix**

[Confusion Matrix Terminologies](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)  
Accuracy: Overall, how often is the classifier correct?  
(TP+TN)/total = (100+50)/165 = 0.91  
Misclassification Rate: Overall, how often is it wrong?  
(FP+FN)/total = (10+5)/165 = 0.09  
equivalent to 1 minus Accuracy
also known as "Error Rate"  
True Positive Rate: When it's actually yes, how often does it predict yes?  
TP/actual yes = 100/105 = 0.95  
also known as "Sensitivity" or "Recall"  
False Positive Rate: When it's actually no, how often does it predict yes?  
FP/actual no = 10/60 = 0.17  
True Negative Rate: When it's actually no, how often does it predict no?  
TN/actual no = 50/60 = 0.83  
equivalent to 1 minus False Positive Rate  
also known as "Specificity"  
Precision: When it predicts yes, how often is it correct?  
TP/predicted yes = 100/110 = 0.91  
Prevalence: How often does the yes condition actually occur in our sample?
actual yes/total = 105/165 = 0.64  
A couple other terms are also worth mentioning:  

Null Error Rate: This is how often you would be wrong if you always predicted the majority class. (In our example, the null error rate would be 60/165=0.36 because if you always predicted yes, you would only be wrong for the 60 "no" cases.) This can be a useful baseline metric to compare your classifier against. However, the best classifier for a particular application will sometimes have a higher error rate than the null error rate, as demonstrated by the Accuracy Paradox.  
Cohen's Kappa: This is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. In other words, a model will have a high Kappa score if there is a big difference between the accuracy and the null error rate. (More details about Cohen's Kappa.)  
F Score: This is a weighted average of the true positive rate (recall) and precision. (More details about the F Score.)  
ROC Curve: This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class. (More details about ROC Curves.)

### **Confusion Matrix**

A `Confusion Matrix` is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict.  
In essence, giving you an Idea of where the Model is getting confused.
"""

from sklearn.metrics  import confusion_matrix
y_preds = clf.predict(X_test)
confusion_matrix(y_test, y_preds)

pd.crosstab(y_test,
            y_preds,
            rownames=["Actual Labels"],
            colnames=["Predicted Labels"])

len(y_preds)

# make a confusion matrix more visual with seaborn's heatmap()

import seaborn as sns

# set the font sale
sns.set(font_scale=1.5)

# create a confusion matrix
conf_mat = confusion_matrix(y_test, y_preds)

# plot it using seaborn
sns.heatmap(conf_mat);

"""## **Evaluating a Classification Model 5 (Confusion Matrix)**"""

from sklearn.metrics import confusion_matrix
y_preds = clf.predict(X_test)
confusion_matrix(y_test, y_preds)

pd.crosstab(y_test, y_preds,
            rownames=["Actual Labels"],
            colnames=["Predicted Labels"])

"""### **Creating a Confusion Matrix using Scikit-Learn**

To use the new mehods of creating a confusion Matrix with Scikit-Learn you will need sklean version 1.0+
"""

# check version of sklearn
import sklearn
sklearn.__version__

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X, y=y)

ConfusionMatrixDisplay.from_predictions(y_true=y_test,
                                       y_pred=y_preds)

"""## **Evaluating a Classification Model 6 (Classification Report)**

### **Classification Report**
"""

from sklearn.metrics import classification_report
print(classification_report(y_test, y_preds))

# where precision and recall become valuabel
disease_true = np.zeros(10000)
disease_true[0] = 1   # only one positive case
disease_preds = np.zeros(10000)  # model predicts everycase as 0
pd.DataFrame(classification_report(disease_true,
                                   disease_preds,
                                   output_dict=True))

"""## **Evaluating a Regression Model 1 (R2 Score)**

**[Model Evaluation Metrics Documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)**
The Ones which we are going to cover are
- R^2 (Pronounced r-squared) of coefficient of determination.
- Mean Absolute Error (MAE)
- Mean Squared Error (MSE)  

What **R-Squared** does : It Compares your model predictions to the mean of targets. Values can range from negative infinity (a very poor model) to 1.   For Example : If all your model does is predict mean of the targets, its R^2 value would be 0, and if your model perfectly predicts the range of numbers its R^2 value will be 1.
"""

from sklearn.ensemble import RandomForestRegressor
np.random.seed(42)

X = housing_df.drop("target", axis=1)
y = housing_df["target"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = RandomForestRegressor()
model.fit(X_train, y_train)

model.score(X_test, y_test)

housing_df.head()

y_test

y_test.mean()

from sklearn.metrics import r2_score

# fill an array with the y_test mean
y_test_mean = np.full(len(y_test), y_test.mean())

y_test_mean[:10]

r2_score(y_true=y_test,
         y_pred=y_test_mean)

r2_score(y_true=y_test,
         y_pred=y_test)

"""## **Evaluating a Regression Model 2 (MAE)**

MAE is tha average of the absolute differences between predictions and actual values. It gives you an Idea of how wrong your model predictions are.
"""

from sklearn.metrics import mean_absolute_error

y_preds = model.predict(X_test)
mae = mean_absolute_error(y_test, y_preds)
mae

y_preds

y_test

df = pd.DataFrame(data={"actual values": y_test,
                       "predicted values": y_preds})
df["differences"] = df["predicted values"] - df["actual values"]
df.head(10)

df["differences"].mean()

# MAE using formulas and differences
np.abs(df["differences"]).mean()

"""## **Evaluating a Regression Model 3 (MSE)**

MSE is the mean of the square of the errors between actual and predicted values.
"""

# Mean Squared Error
from sklearn.metrics import mean_squared_error

y_preds = model.predict(X_test)
mse = mean_squared_error(y_test, y_preds)
mse

df["squared differences"] = np.square(df["differences"])
df.head()

# Calculate MSE by hand
squared = np.square(df["differences"])
squared.mean()

df_large_error = df.copy()
df_large_error.iloc[0]["squared differences"] = 16

df_large_error.head()

df_large_error["differences"].mean()

"""## **Evaluating a Model with Cross Validation and Scoring Parameter**"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

clf = RandomForestClassifier(n_estimators=100)

np.random.seed(42)

# Cross validation Accuracy
cv_acc = cross_val_score(clf, X, y, cv=5, scoring=None) # If scoring = None estimators default scoring metrics is used which is accuracy for classification model

cv_acc

# Cross Validated Accuracy
print(f"The Cross validated Accuracy is : {np.mean(cv_acc)*100:.2f}%")

np.random.seed(42)
cv_acc = cross_val_score(clf, X, y, cv=5, scoring="accuracy")
cv_acc

# Cross Validated Accuracy
print(f"The Cross validated Accuracy is : {np.mean(cv_acc)*100:.2f}%")

# Precision
np.random.seed(42)
cv_precision = cross_val_score(clf, X, y, cv=5, scoring="precision")
cv_precision

# Cross Validated Precision
print(f"The Cross validated Precision is : {np.mean(cv_precision)*100:.2f}%")

# Recall
np.random.seed(42)
cv_recall = cross_val_score(clf, X, y, cv=5, scoring="recall")
cv_recall

# Cross Validated Recall
print(f"The Cross validated Recall is : {np.mean(cv_recall)*100:.2f}%")

"""Let's see the `scoring` parameter being used for Regression Problem"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor

np.random.seed(42)

X = housing_df.drop("target", axis=1)
y = housing_df["target"]

model = RandomForestRegressor(n_estimators=100)

np.random.seed(42)
cv_r2 = cross_val_score(model, X, y, cv=3, scoring=None)
np.mean(cv_r2)

cv_r2

# Mean Absolute Error
cv_mae = cross_val_score(model, X, y, cv=3, scoring="neg_mean_absolute_error")
np.mean(cv_mae)

cv_mae

# Mean Squared Error
cv_mse = cross_val_score(model, X, y, cv=3, scoring="neg_mean_squared_error")
np.mean(cv_mse)

cv_mse

"""## **Evaluating a Model with Scikit-Learn Functions**

The 3rd way to evaluate the Scikit-Learn Machine Learning Models/Estimators int the sklearn.metrics module  
- **[sklearn.metrics module](https://scikit-learn.org/stable/modules/model_evaluation.html)**
"""

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

np.random.seed(42)

# create X and y
X = heart_disease.drop("target", axis=1)
y = heart_disease["target"]

# split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# create Model
clf = RandomForestClassifier()

# fit model
clf.fit(X_train, y_train)

# make predictions
y_preds = clf.predict(X_test)

# evaluate model using evaluation functions
print("Classifier Metrics on the Test Set")
print(f"Accuracy : {accuracy_score(y_test, y_preds)*100:.2f}%")
print(f"Precision : {precision_score(y_test, y_preds)}")
print(f"Recall : {recall_score(y_test, y_preds)}")
print(f"F1 Score : {f1_score(y_test, y_preds)}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

np.random.seed(42)

# create X and y
X = housing_df.drop("target", axis=1)
y = housing_df["target"]

# split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# create Model
clf = RandomForestRegressor()

# fit model
model.fit(X_train, y_train)

# make predictions
y_preds = model.predict(X_test)

# evaluate model using evaluation functions
print("Regression Metrics on the Test Set")
print(f"R2 Score : {r2_score(y_test, y_preds)}")
print(f"Mean Absolute Error(MAE) : {mean_absolute_error(y_test, y_preds)}")
print(f"Mean Squared Error(MSE) : {mean_squared_error(y_test, y_preds)}")

"""## **Improving a Machine Learning Model**

First Predictions = Bseline Predictions  
First Model = Baseline Model

From a `Data` Perspective
- Could we collect more Data ? (Generally, the more Data, the better)
- Could we improve our Data ?

From a `Model` Perspective :
- Is their a better model to use ?
- Could we improve the current Model ?  

*Hyperparameters vs. Parameters*  
Parameters = Model Fins these Patterns in data  
Hyperparameters = Settings on a Model you can adjust to (potentially) potentially improves its ability to find patterns  

Three Ways to Adjust Hyperparameters  
1. By Hand  
2. Randomly with RandomSearchCV
3. Exhaustively with GridSearchCV
"""

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
clf.get_params()

"""## **Tuning Hyperparameters**

### **Tuning Hyperparameters by Hand**

Lets make 3 sets, training, validation and test sets
"""

clf.get_params()

"""We are going to Try and Adjust
- `max_depth`
- `max_features`
- `min_samples_leaf`
- `min_samples_split`
- `n_estimators`
"""

def evaluate_preds(y_true, y_preds):
  """
  Performs Evaluation Comparison on y_true labels vs y_pred labels
  """
  accuracy = accuracy_score(y_true, y_preds)
  precision = precision_score(y_true, y_preds)
  recall = recall_score(y_true, y_preds)
  f1 = f1_score(y_true, y_preds)
  metric_dict = {"accuracy": round(accuracy, 2),
                 "precision": round(precision, 2),
                 "recall": round(recall, 2),
                 "f1": round(f1, 2)}
  print(f"Accuracy : {accuracy * 100:.2f}%")
  print(f"Precision : {precision:.2f}")
  print(f"Recall : {recall:.2f}")
  print(f"F1 Score : {f1:.2f}")

  return metric_dict

from sklearn.ensemble import RandomForestClassifier

np.random.seed(42)

# shuffle the data
heart_disease_shuffled = heart_disease.sample(frac=1)

# split into X and y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# split the data into train, validation and test sets
train_split = round(0.7 * len(heart_disease_shuffled))   # 70% of the data
valid_split = round(train_split + 0.15 * len(heart_disease_shuffled))    # 15% of the Data
X_train, y_train = X[:train_split], y[:train_split]
X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]
X_test, y_test = X[valid_split:], y[valid_split:]

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Make Baseline predictions
y_preds = clf.predict(X_valid)

# Evaluate the classifier on Validation Set
baseline_metrics = evaluate_preds(y_valid, y_preds)
baseline_metrics

np.random.seed(42)

# create a second classifier with different hyperparameters
clf_2 = RandomForestClassifier(n_estimators=100)
clf_2.fit(X_train, y_train)

# make predictions with different hyperparameters
y_preds_2 = clf_2.predict(X_valid)

# evaluate the second classifier
clf_2_metrics = evaluate_preds(y_valid, y_preds_2)

clf_3 = RandomForestClassifier(n_estimators=100,
                               max_depth=10)

"""## **Tuning Hyperparameters 2**

### **Hyperparameter Tuning with RandomizedSearchCV**
"""

from sklearn.model_selection import RandomizedSearchCV
grid = {"n_estimators":[10, 100, 200, 500, 1000, 1200],
        "max_depth": [None, 5, 10, 20, 30],
        "max_features":["auto", "sqrt"],
        "min_samples_split":[2, 4, 6],
        "min_samples_leaf": [1, 2, 4]}

np.random.seed(42)

# split into X and y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# split into train and test splits
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# setup RandomizedSearchCV
rs_clf = RandomizedSearchCV(estimator=clf,
                            param_distributions=grid,
                            n_iter=10,          # No. of models to try
                            cv=5,               
                            verbose=2)

# fit the RandomizedSearchCV version of clf
rs_clf.fit(X_train, y_train)

rs_clf.best_params_   # gives out the best parameters out of our combination

# Make predictions with the bast hyperparameters
rs_y_preds = rs_clf.predict(X_test)

# Evaluate the predictiosn
rs_metrics = evaluate_preds(y_test, rs_y_preds)

"""## **Tuning Hyperparameters 3**

### **Hyperparameter Tuning with GridSearchCV**
"""

grid

grid_2 = {'n_estimators': [100, 200, 500],
          'max_depth': [None],
          'max_features': ['auto', 'sqrt'],
          'min_samples_split': [6],
          'min_samples_leaf': [1, 2]}

from sklearn.model_selection import GridSearchCV, train_test_split

np.random.seed(42)

# split into X and y
X = heart_disease_shuffled.drop("target", axis=1)
y = heart_disease_shuffled["target"]

# split into train and test splits
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# instantiate RandomForestClassifier
clf = RandomForestClassifier(n_jobs=1)

# setup GridSearchCV
gs_clf = GridSearchCV(estimator=clf,
                      param_grid=grid_2,
                      cv=5,               
                      verbose=2)

# fit the GridSearchCV version of clf
gs_clf.fit(X_train, y_train)

gs_clf.best_params_

gs_y_preds = gs_clf.predict(X_test)

# evaluate the predictions
gs_metrics = evaluate_preds(y_test, gs_y_preds)

"""Let's Compare our Different Model Metrics"""

compare_metrics = pd.DataFrame({"baseline": baseline_metrics,
                               "clf_2": clf_2_metrics,
                               "random_search": rs_metrics,
                               "grid_search": gs_metrics})
compare_metrics.plot.bar(figsize=(10, 8));

"""## **Saving and Loading a Model**

**Two ways to save and load Machine Learning Model**
1. With python's `pickle` module
2. With the `joblib` module

### **`pickle` Module**
"""

import pickle

# Save an existing model to a file
pickle.dump(gs_clf, open("gs_random_forest_model_1.pkl", "wb"))

# Load a Saved model
loaded_pickle_model = pickle.load(open("gs_random_forest_model_1.pkl", "rb"))

# make and evaluate
pickle_y_preds = loaded_pickle_model.predict(X_test)
evaluate_preds(y_test, pickle_y_preds)

"""### **`joblib` Module**"""

from joblib import dump, load
# save a model to a file
dump(gs_clf, filename="gs_random_forest_model_1.joblib")

# import a saved loblib model
loaded_joblib_model = load(filename="gs_random_forest_model_1.joblib")

# make and evaluate
joblib_y_preds = loaded_joblib_model.predict(X_test)
evaluate_preds(y_test, joblib_y_preds)

"""## **Putting it all Together**"""

data = pd.read_csv("/content/car-sales-extended-missing-data.csv")
data

data.dtypes

data.isna

data.isna().sum()

# Getting data ready
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Modelling
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV

# Setup random seed
import numpy as np
np.random.seed(42)

# Import data and drop rows with missing labels
data = pd.read_csv("/content/car-sales-extended-missing-data.csv")
data.dropna(subset=["Price"], inplace=True)

# Define different features and transformer pipeline
categorical_features = ["Make", "Colour"]
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value="missing")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))])

door_feature = ["Doors"]
door_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="constant", fill_value=4))
])

numeric_features = ["Odometer (KM)"]
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean"))
])

# Setup preprocessing steps (fill missing values, then convert to numbers)
preprocessor = ColumnTransformer(
                    transformers=[
                        ("cat", categorical_transformer, categorical_features),
                        ("door", door_transformer, door_feature),
                        ("num", numeric_transformer, numeric_features)
                    ])

# Creating a preprocessing and modelling pipeline
model = Pipeline(steps=[("preprocessor", preprocessor),
                        ("model", RandomForestRegressor())])

# Split data
X = data.drop("Price", axis=1)
y = data["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Fit and score the model
model.fit(X_train, y_train)
model.score(X_test, y_test)

# Use GridSearchCV with our regression Pipeline
from sklearn.model_selection import GridSearchCV

pipe_grid = {
    "preprocessor__num__imputer__strategy": ["mean", "median"],
    "model__n_estimators": [100, 1000],
    "model__max_depth": [None, 5],
    "model__max_features": ["auto"],
    "model__min_samples_split": [2, 4]    
}

gs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2)
gs_model.fit(X_train, y_train)

gs_model.score(X_test, y_test)

